{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from llm.gemini import Gemini\n",
    "from llm.llm_utils import *\n",
    "#\n",
    "EMBEDDING_MODEL_NAME = 'bkai-foundation-models/vietnamese-bi-encoder'\n",
    "\n",
    "embd = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM for summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "raptor_client = chromadb.PersistentClient(path='database/raptor_2.db')\n",
    "raptor_db = Chroma(client=raptor_client, embedding_function=embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "\n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    "\n",
    "### --- Code from citations referenced above (added comments and docstrings) --- ###\n",
    "\n",
    "\n",
    "def global_cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    n_neighbors: Optional[int] = None,\n",
    "    metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    \n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "def local_cluster_embeddings(\n",
    "    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \n",
    "    \n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "def get_optimal_clusters(\n",
    "    embeddings: np.ndarray, max_clusters: int = 400, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \n",
    "    \n",
    "    max_clusters = min(max_clusters, int(len(embeddings)/2))\n",
    "\n",
    "\n",
    "    if max_clusters >= 300:\n",
    "        n_clusters = np.arange(50, max_clusters+1,10)\n",
    "    elif max_clusters > 100:\n",
    "        n_clusters = np.arange(1, max_clusters,2)\n",
    "    else:\n",
    "        n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    \n",
    "    if n_clusters[0] == 400:\n",
    "        return 400\n",
    "    for n in tqdm(n_clusters, desc=\"Optimizing clusters length: \"+str(len(embeddings))):\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    # print(max_clusters,\" : \",n_clusters[np.argmin(bics)])\n",
    "    return n_clusters[np.argmin(bics)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \n",
    "    \n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    "\n",
    "\n",
    "def perform_clustering(\n",
    "    embeddings: np.ndarray,\n",
    "    dim: int,\n",
    "    threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \n",
    "\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    "\n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    "\n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    "\n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    "\n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    "\n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    "\n",
    "        total_clusters += n_local_clusters\n",
    "\n",
    "    return all_local_clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Our code below --- ###\n",
    "import time\n",
    "\n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of text documents.\n",
    "\n",
    "    This function assumes the existence of an `embd` object with a method `embed_documents`\n",
    "    that takes a list of texts and returns their embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be embedded.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array of embeddings for the given text documents.\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    "\n",
    "\n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n",
    "\n",
    "    This function combines embedding generation and clustering into a single step. It assumes the existence\n",
    "    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], a list of text documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 15, 0.11\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Formats the text documents in a DataFrame into a single string.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the 'text' column with text documents to format.\n",
    "\n",
    "    Returns:\n",
    "    - A single string where all text documents are joined by a specific delimiter.\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt).replace(\"_\",\" \")\n",
    "\n",
    "\n",
    "def embed_cluster_summarize_texts(\n",
    "    texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \n",
    "\n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    "\n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    "\n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    "\n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    "\n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    "\n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    "\n",
    "    # Summarization\n",
    "    template = \"\"\"\n",
    "Bạn là một AI được huấn luyện trong việc tóm tắt văn bản. Bạn được cung cấp các thông tin hữu ích sau đây. \n",
    "    {context}\n",
    "Hãy viết một văn bản mới có nội tóm tắt và trích xuất nội dung quan trọng có trong các văn bản trên trong khoảng 100-150 từ\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    bs = 10\n",
    "    batch_i =[]\n",
    "    \n",
    "    for i in range(0, len(all_clusters), bs):\n",
    "        batch_i.append(all_clusters[i:i+bs])\n",
    "    take_cluster = []\n",
    "    for i in all_clusters:\n",
    "        try:\n",
    "            print(f\"Summarizing cluster {i}...\")\n",
    "            df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "            formatted_txt = fmt_txt(df_cluster)\n",
    "            message = template.replace(\"{context}\", formatted_txt)\n",
    "            messages = [{\"role\":\"user\", \"content\":message}]\n",
    "            response = llm(messages)\n",
    "            with open(\"text2.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(response)\n",
    "                f.write(\"=====================================\\n\")\n",
    "            summaries.append(response)\n",
    "            take_cluster.append(i)\n",
    "        except:\n",
    "            print(\"Error\")\n",
    "            continue\n",
    "\n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": take_cluster,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return df_clusters, df_summary\n",
    "\n",
    "\n",
    "def recursive_embed_cluster_summarize(\n",
    "    texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n",
    "    the number of unique clusters becomes 1, storing the results at each level.\n",
    "\n",
    "    Parameters:\n",
    "    - texts: List[str], texts to be processed.\n",
    "    - level: int, current recursion level (starts at 1).\n",
    "    - n_levels: int, maximum depth of recursion.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n",
    "      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    "\n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "\n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "\n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    "\n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "leaf_texts = set()\n",
    "for file in os.listdir('chunks'):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(f'chunks/{file}', 'r') as f:\n",
    "            text = f.read()\n",
    "            texts = text_splitter.split_text(text)\n",
    "            # results = recursive_embed_cluster_summarize(texts, n_levels=3)\n",
    "            # print(results)\n",
    "            for text in texts:\n",
    "                leaf_texts.add(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_texts = list(leaf_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4526"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing clusters length: 4526: 100%|██████████| 36/36 [05:02<00:00,  8.39s/it]\n",
      "Optimizing clusters length: 91: 100%|██████████| 44/44 [00:02<00:00, 15.03it/s]\n",
      "Optimizing clusters length: 80: 100%|██████████| 39/39 [00:02<00:00, 15.73it/s]\n",
      "Optimizing clusters length: 86: 100%|██████████| 42/42 [00:02<00:00, 17.63it/s]\n",
      "Optimizing clusters length: 83: 100%|██████████| 40/40 [00:02<00:00, 16.84it/s]\n",
      "Optimizing clusters length: 104: 100%|██████████| 51/51 [00:03<00:00, 16.72it/s]\n",
      "Optimizing clusters length: 160: 100%|██████████| 79/79 [00:05<00:00, 14.32it/s]\n",
      "Optimizing clusters length: 65: 100%|██████████| 31/31 [00:02<00:00, 14.26it/s]\n",
      "Optimizing clusters length: 83: 100%|██████████| 40/40 [00:02<00:00, 16.52it/s]\n",
      "Optimizing clusters length: 103: 100%|██████████| 50/50 [00:02<00:00, 18.84it/s]\n",
      "Optimizing clusters length: 80: 100%|██████████| 39/39 [00:02<00:00, 16.12it/s]\n",
      "Optimizing clusters length: 68: 100%|██████████| 33/33 [00:01<00:00, 21.32it/s]\n",
      "Optimizing clusters length: 146: 100%|██████████| 72/72 [00:05<00:00, 12.66it/s]\n",
      "Optimizing clusters length: 99: 100%|██████████| 48/48 [00:03<00:00, 13.90it/s]\n",
      "Optimizing clusters length: 72: 100%|██████████| 35/35 [00:02<00:00, 16.52it/s]\n",
      "Optimizing clusters length: 135: 100%|██████████| 66/66 [00:04<00:00, 14.87it/s]\n",
      "Optimizing clusters length: 147: 100%|██████████| 72/72 [00:05<00:00, 13.48it/s]\n",
      "Optimizing clusters length: 150: 100%|██████████| 74/74 [00:05<00:00, 12.63it/s]\n",
      "Optimizing clusters length: 93: 100%|██████████| 45/45 [00:02<00:00, 16.51it/s]\n",
      "Optimizing clusters length: 125: 100%|██████████| 61/61 [00:04<00:00, 15.09it/s]\n",
      "Optimizing clusters length: 114: 100%|██████████| 56/56 [00:03<00:00, 16.04it/s]\n",
      "Optimizing clusters length: 100: 100%|██████████| 49/49 [00:03<00:00, 15.57it/s]\n",
      "Optimizing clusters length: 106: 100%|██████████| 52/52 [00:03<00:00, 16.89it/s]\n",
      "Optimizing clusters length: 90: 100%|██████████| 44/44 [00:02<00:00, 16.18it/s]\n",
      "Optimizing clusters length: 94: 100%|██████████| 46/46 [00:03<00:00, 14.96it/s]\n",
      "Optimizing clusters length: 70: 100%|██████████| 34/34 [00:01<00:00, 17.51it/s]\n",
      "Optimizing clusters length: 61: 100%|██████████| 29/29 [00:01<00:00, 19.77it/s]\n",
      "Optimizing clusters length: 67: 100%|██████████| 32/32 [00:02<00:00, 15.89it/s]\n",
      "Optimizing clusters length: 59: 100%|██████████| 28/28 [00:01<00:00, 18.69it/s]\n",
      "Optimizing clusters length: 80: 100%|██████████| 39/39 [00:02<00:00, 15.89it/s]\n",
      "Optimizing clusters length: 44: 100%|██████████| 21/21 [00:01<00:00, 17.24it/s]\n",
      "Optimizing clusters length: 67: 100%|██████████| 32/32 [00:02<00:00, 13.80it/s]\n",
      "Optimizing clusters length: 85: 100%|██████████| 41/41 [00:02<00:00, 16.35it/s]\n",
      "Optimizing clusters length: 106: 100%|██████████| 52/52 [00:03<00:00, 15.99it/s]\n",
      "Optimizing clusters length: 66: 100%|██████████| 32/32 [00:01<00:00, 19.46it/s]\n",
      "Optimizing clusters length: 92: 100%|██████████| 45/45 [00:02<00:00, 16.09it/s]\n",
      "Optimizing clusters length: 85: 100%|██████████| 41/41 [00:02<00:00, 17.05it/s]\n",
      "Optimizing clusters length: 75: 100%|██████████| 36/36 [00:02<00:00, 15.52it/s]\n",
      "Optimizing clusters length: 83: 100%|██████████| 40/40 [00:02<00:00, 15.29it/s]\n",
      "Optimizing clusters length: 148: 100%|██████████| 73/73 [00:05<00:00, 13.95it/s]\n",
      "Optimizing clusters length: 73: 100%|██████████| 35/35 [00:02<00:00, 14.67it/s]\n",
      "Optimizing clusters length: 124: 100%|██████████| 61/61 [00:04<00:00, 14.81it/s]\n",
      "Optimizing clusters length: 84: 100%|██████████| 41/41 [00:02<00:00, 14.04it/s]\n",
      "Optimizing clusters length: 89: 100%|██████████| 43/43 [00:02<00:00, 15.18it/s]\n",
      "Optimizing clusters length: 92: 100%|██████████| 45/45 [00:02<00:00, 16.34it/s]\n",
      "Optimizing clusters length: 78: 100%|██████████| 38/38 [00:02<00:00, 15.33it/s]\n",
      "Optimizing clusters length: 69: 100%|██████████| 33/33 [00:01<00:00, 17.92it/s]\n",
      "Optimizing clusters length: 158: 100%|██████████| 78/78 [00:05<00:00, 14.32it/s]\n",
      "Optimizing clusters length: 64: 100%|██████████| 31/31 [00:01<00:00, 19.90it/s]\n",
      "Optimizing clusters length: 83: 100%|██████████| 40/40 [00:02<00:00, 17.36it/s]\n",
      "Optimizing clusters length: 71: 100%|██████████| 34/34 [00:01<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 274 clusters--\n",
      "Summarizing cluster 62.0...\n",
      "Summarizing cluster 85.0...\n",
      "Summarizing cluster 63.0...\n",
      "Summarizing cluster 124.0...\n",
      "Summarizing cluster 167.0...\n",
      "Summarizing cluster 101.0...\n",
      "Summarizing cluster 248.0...\n",
      "Summarizing cluster 245.0...\n",
      "Summarizing cluster 227.0...\n",
      "Summarizing cluster 29.0...\n",
      "Summarizing cluster 81.0...\n",
      "Summarizing cluster 27.0...\n",
      "Summarizing cluster 263.0...\n",
      "Summarizing cluster 30.0...\n",
      "Summarizing cluster 169.0...\n",
      "Summarizing cluster 220.0...\n",
      "Summarizing cluster 196.0...\n",
      "Summarizing cluster 223.0...\n",
      "Summarizing cluster 40.0...\n",
      "Summarizing cluster 250.0...\n",
      "Summarizing cluster 273.0...\n",
      "Summarizing cluster 59.0...\n",
      "Summarizing cluster 113.0...\n",
      "Summarizing cluster 122.0...\n",
      "Summarizing cluster 259.0...\n",
      "Summarizing cluster 202.0...\n",
      "Summarizing cluster 266.0...\n",
      "Summarizing cluster 146.0...\n",
      "Summarizing cluster 157.0...\n",
      "Summarizing cluster 86.0...\n",
      "Summarizing cluster 7.0...\n",
      "Summarizing cluster 25.0...\n",
      "Summarizing cluster 26.0...\n",
      "Summarizing cluster 11.0...\n",
      "Summarizing cluster 3.0...\n",
      "Summarizing cluster 106.0...\n",
      "Summarizing cluster 175.0...\n",
      "Summarizing cluster 74.0...\n",
      "Summarizing cluster 52.0...\n",
      "Summarizing cluster 192.0...\n",
      "Summarizing cluster 136.0...\n",
      "Summarizing cluster 9.0...\n",
      "Summarizing cluster 262.0...\n",
      "Summarizing cluster 44.0...\n",
      "Summarizing cluster 77.0...\n",
      "Summarizing cluster 20.0...\n",
      "Summarizing cluster 177.0...\n",
      "Summarizing cluster 89.0...\n",
      "Summarizing cluster 6.0...\n",
      "Summarizing cluster 184.0...\n",
      "Summarizing cluster 42.0...\n",
      "Summarizing cluster 253.0...\n",
      "Summarizing cluster 148.0...\n",
      "Summarizing cluster 66.0...\n",
      "Summarizing cluster 82.0...\n",
      "Summarizing cluster 252.0...\n",
      "Summarizing cluster 114.0...\n",
      "Summarizing cluster 28.0...\n",
      "Summarizing cluster 186.0...\n",
      "Summarizing cluster 218.0...\n",
      "Summarizing cluster 159.0...\n",
      "Summarizing cluster 132.0...\n",
      "Summarizing cluster 257.0...\n",
      "Summarizing cluster 108.0...\n",
      "Summarizing cluster 56.0...\n",
      "Summarizing cluster 65.0...\n",
      "Summarizing cluster 205.0...\n",
      "Summarizing cluster 95.0...\n",
      "Summarizing cluster 154.0...\n",
      "Summarizing cluster 243.0...\n",
      "Summarizing cluster 58.0...\n",
      "Summarizing cluster 198.0...\n",
      "Summarizing cluster 161.0...\n",
      "Summarizing cluster 139.0...\n",
      "Summarizing cluster 131.0...\n",
      "Summarizing cluster 60.0...\n",
      "Summarizing cluster 45.0...\n",
      "Summarizing cluster 38.0...\n",
      "Summarizing cluster 173.0...\n",
      "Summarizing cluster 249.0...\n",
      "Summarizing cluster 76.0...\n",
      "Summarizing cluster 91.0...\n",
      "Summarizing cluster 215.0...\n",
      "Summarizing cluster 256.0...\n",
      "Summarizing cluster 102.0...\n",
      "Summarizing cluster 53.0...\n",
      "Summarizing cluster 200.0...\n",
      "Summarizing cluster 145.0...\n",
      "Summarizing cluster 204.0...\n",
      "Summarizing cluster 43.0...\n",
      "Summarizing cluster 93.0...\n",
      "Summarizing cluster 178.0...\n",
      "Summarizing cluster 170.0...\n",
      "Summarizing cluster 13.0...\n",
      "Summarizing cluster 189.0...\n",
      "Summarizing cluster 17.0...\n",
      "Summarizing cluster 103.0...\n",
      "Summarizing cluster 121.0...\n",
      "Summarizing cluster 216.0...\n",
      "Summarizing cluster 79.0...\n",
      "Summarizing cluster 138.0...\n",
      "Summarizing cluster 18.0...\n",
      "Summarizing cluster 236.0...\n",
      "Summarizing cluster 144.0...\n",
      "Summarizing cluster 100.0...\n",
      "Summarizing cluster 39.0...\n",
      "Summarizing cluster 191.0...\n",
      "Summarizing cluster 201.0...\n",
      "Summarizing cluster 98.0...\n",
      "Summarizing cluster 209.0...\n",
      "Summarizing cluster 14.0...\n",
      "Summarizing cluster 166.0...\n",
      "Summarizing cluster 54.0...\n",
      "Summarizing cluster 163.0...\n",
      "Summarizing cluster 199.0...\n",
      "Summarizing cluster 34.0...\n",
      "Summarizing cluster 271.0...\n",
      "Summarizing cluster 212.0...\n",
      "Summarizing cluster 129.0...\n",
      "Summarizing cluster 123.0...\n",
      "Summarizing cluster 48.0...\n",
      "Summarizing cluster 247.0...\n",
      "Summarizing cluster 254.0...\n",
      "Summarizing cluster 168.0...\n",
      "Summarizing cluster 35.0...\n",
      "Summarizing cluster 155.0...\n",
      "Summarizing cluster 156.0...\n",
      "Summarizing cluster 46.0...\n",
      "Summarizing cluster 120.0...\n",
      "Summarizing cluster 219.0...\n",
      "Summarizing cluster 92.0...\n",
      "Summarizing cluster 119.0...\n",
      "Summarizing cluster 207.0...\n",
      "Summarizing cluster 234.0...\n",
      "Summarizing cluster 1.0...\n",
      "Summarizing cluster 244.0...\n",
      "Summarizing cluster 142.0...\n",
      "Summarizing cluster 241.0...\n",
      "Summarizing cluster 23.0...\n",
      "Summarizing cluster 69.0...\n",
      "Summarizing cluster 130.0...\n",
      "Summarizing cluster 269.0...\n",
      "Summarizing cluster 83.0...\n",
      "Summarizing cluster 115.0...\n",
      "Summarizing cluster 224.0...\n",
      "Summarizing cluster 150.0...\n",
      "Summarizing cluster 221.0...\n",
      "Summarizing cluster 143.0...\n",
      "Summarizing cluster 31.0...\n",
      "Summarizing cluster 72.0...\n",
      "Summarizing cluster 180.0...\n",
      "Summarizing cluster 94.0...\n",
      "Summarizing cluster 4.0...\n",
      "Summarizing cluster 37.0...\n",
      "Summarizing cluster 258.0...\n",
      "Summarizing cluster 55.0...\n",
      "Summarizing cluster 99.0...\n",
      "Summarizing cluster 97.0...\n",
      "Summarizing cluster 187.0...\n",
      "Summarizing cluster 19.0...\n",
      "Summarizing cluster 272.0...\n",
      "Summarizing cluster 90.0...\n",
      "Summarizing cluster 2.0...\n",
      "Summarizing cluster 107.0...\n",
      "Summarizing cluster 32.0...\n",
      "Summarizing cluster 84.0...\n",
      "Summarizing cluster 153.0...\n",
      "Summarizing cluster 225.0...\n",
      "Summarizing cluster 96.0...\n",
      "Summarizing cluster 270.0...\n",
      "Summarizing cluster 61.0...\n",
      "Summarizing cluster 70.0...\n",
      "Summarizing cluster 193.0...\n",
      "Summarizing cluster 151.0...\n",
      "Summarizing cluster 171.0...\n",
      "Summarizing cluster 57.0...\n",
      "Summarizing cluster 183.0...\n",
      "Summarizing cluster 152.0...\n",
      "Summarizing cluster 33.0...\n",
      "Summarizing cluster 5.0...\n",
      "Summarizing cluster 41.0...\n",
      "Summarizing cluster 8.0...\n",
      "Summarizing cluster 15.0...\n",
      "Summarizing cluster 237.0...\n",
      "Summarizing cluster 24.0...\n",
      "Summarizing cluster 88.0...\n",
      "Summarizing cluster 127.0...\n",
      "Summarizing cluster 246.0...\n",
      "Summarizing cluster 105.0...\n",
      "Summarizing cluster 78.0...\n",
      "Summarizing cluster 47.0...\n",
      "Summarizing cluster 233.0...\n",
      "Summarizing cluster 176.0...\n",
      "Summarizing cluster 50.0...\n",
      "Summarizing cluster 255.0...\n",
      "Summarizing cluster 235.0...\n",
      "Summarizing cluster 229.0...\n",
      "Summarizing cluster 80.0...\n",
      "Summarizing cluster 10.0...\n",
      "Summarizing cluster 134.0...\n",
      "Summarizing cluster 203.0...\n",
      "Summarizing cluster 160.0...\n",
      "Summarizing cluster 140.0...\n",
      "Summarizing cluster 267.0...\n",
      "Summarizing cluster 213.0...\n",
      "Summarizing cluster 104.0...\n",
      "Summarizing cluster 117.0...\n",
      "Summarizing cluster 137.0...\n",
      "Summarizing cluster 75.0...\n",
      "Summarizing cluster 125.0...\n",
      "Summarizing cluster 214.0...\n",
      "Summarizing cluster 149.0...\n",
      "Summarizing cluster 181.0...\n",
      "Summarizing cluster 164.0...\n",
      "Summarizing cluster 261.0...\n",
      "Summarizing cluster 87.0...\n",
      "Summarizing cluster 228.0...\n",
      "Summarizing cluster 128.0...\n",
      "Summarizing cluster 174.0...\n",
      "Summarizing cluster 232.0...\n",
      "Summarizing cluster 36.0...\n",
      "Summarizing cluster 185.0...\n",
      "Summarizing cluster 16.0...\n",
      "Summarizing cluster 208.0...\n",
      "Summarizing cluster 188.0...\n",
      "Summarizing cluster 206.0...\n",
      "Summarizing cluster 260.0...\n",
      "Summarizing cluster 111.0...\n",
      "Summarizing cluster 22.0...\n",
      "Summarizing cluster 49.0...\n",
      "Summarizing cluster 242.0...\n",
      "Summarizing cluster 64.0...\n",
      "Summarizing cluster 0.0...\n",
      "Summarizing cluster 190.0...\n",
      "Summarizing cluster 217.0...\n",
      "Summarizing cluster 110.0...\n",
      "Summarizing cluster 230.0...\n",
      "Summarizing cluster 226.0...\n",
      "Summarizing cluster 68.0...\n",
      "Summarizing cluster 210.0...\n",
      "Summarizing cluster 118.0...\n",
      "Summarizing cluster 222.0...\n",
      "Summarizing cluster 251.0...\n",
      "Summarizing cluster 71.0...\n",
      "Summarizing cluster 158.0...\n",
      "Summarizing cluster 116.0...\n",
      "Summarizing cluster 239.0...\n",
      "Summarizing cluster 147.0...\n",
      "Summarizing cluster 231.0...\n",
      "Summarizing cluster 264.0...\n",
      "Summarizing cluster 197.0...\n",
      "Summarizing cluster 179.0...\n",
      "Summarizing cluster 21.0...\n",
      "Summarizing cluster 195.0...\n",
      "Summarizing cluster 265.0...\n",
      "Summarizing cluster 73.0...\n",
      "Summarizing cluster 240.0...\n",
      "Summarizing cluster 172.0...\n",
      "Summarizing cluster 12.0...\n",
      "Summarizing cluster 126.0...\n",
      "Summarizing cluster 211.0...\n",
      "Summarizing cluster 268.0...\n",
      "Summarizing cluster 51.0...\n",
      "Summarizing cluster 238.0...\n",
      "Summarizing cluster 141.0...\n",
      "Summarizing cluster 182.0...\n",
      "Summarizing cluster 133.0...\n",
      "Summarizing cluster 162.0...\n",
      "Summarizing cluster 165.0...\n",
      "Summarizing cluster 112.0...\n",
      "Summarizing cluster 67.0...\n",
      "Summarizing cluster 194.0...\n",
      "Summarizing cluster 109.0...\n",
      "Summarizing cluster 135.0...\n",
      "Level 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing clusters length: 274: 100%|██████████| 68/68 [00:15<00:00,  4.25it/s]\n",
      "Optimizing clusters length: 39: 100%|██████████| 18/18 [00:00<00:00, 21.52it/s]\n",
      "Optimizing clusters length: 44: 100%|██████████| 21/21 [00:00<00:00, 21.10it/s]\n",
      "Optimizing clusters length: 37: 100%|██████████| 17/17 [00:00<00:00, 19.30it/s]\n",
      "Optimizing clusters length: 22: 100%|██████████| 10/10 [00:00<00:00, 19.70it/s]\n",
      "Optimizing clusters length: 33: 100%|██████████| 15/15 [00:00<00:00, 16.61it/s]\n",
      "Optimizing clusters length: 40: 100%|██████████| 19/19 [00:00<00:00, 19.58it/s]\n",
      "Optimizing clusters length: 59: 100%|██████████| 28/28 [00:01<00:00, 20.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 37 clusters--\n",
      "Summarizing cluster 1.0...\n",
      "Summarizing cluster 22.0...\n",
      "Summarizing cluster 28.0...\n",
      "Summarizing cluster 11.0...\n",
      "Summarizing cluster 20.0...\n",
      "Summarizing cluster 7.0...\n",
      "Summarizing cluster 36.0...\n",
      "Summarizing cluster 5.0...\n",
      "Summarizing cluster 34.0...\n",
      "Summarizing cluster 10.0...\n",
      "Summarizing cluster 19.0...\n",
      "Summarizing cluster 31.0...\n",
      "Summarizing cluster 17.0...\n",
      "Summarizing cluster 15.0...\n",
      "Summarizing cluster 8.0...\n",
      "Summarizing cluster 13.0...\n",
      "Summarizing cluster 33.0...\n",
      "Summarizing cluster 27.0...\n",
      "Summarizing cluster 24.0...\n",
      "Summarizing cluster 3.0...\n",
      "Summarizing cluster 23.0...\n",
      "Summarizing cluster 18.0...\n",
      "Summarizing cluster 30.0...\n",
      "Summarizing cluster 32.0...\n",
      "Summarizing cluster 25.0...\n",
      "Summarizing cluster 6.0...\n",
      "Summarizing cluster 14.0...\n",
      "Summarizing cluster 2.0...\n",
      "Summarizing cluster 16.0...\n",
      "Summarizing cluster 0.0...\n",
      "Summarizing cluster 21.0...\n",
      "Summarizing cluster 4.0...\n",
      "Summarizing cluster 12.0...\n",
      "Summarizing cluster 29.0...\n",
      "Summarizing cluster 9.0...\n",
      "Summarizing cluster 26.0...\n",
      "Summarizing cluster 35.0...\n",
      "Level 2 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimizing clusters length: 37: 100%|██████████| 17/17 [00:01<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Generated 4 clusters--\n",
      "Summarizing cluster 1.0...\n",
      "Summarizing cluster 2.0...\n",
      "Summarizing cluster 0.0...\n",
      "Summarizing cluster 3.0...\n",
      "Level 3 completed\n",
      "--Generated 1 clusters--\n",
      "Summarizing cluster 0...\n",
      "Level 4 completed\n"
     ]
    }
   ],
   "source": [
    "stack = [(leaf_texts, 1)]\n",
    "n_levels = 7\n",
    "results = {}\n",
    "while stack:\n",
    "    texts, level = stack.pop()\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    "    results[level] = (df_clusters, df_summary)\n",
    "    \n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        stack.append((new_texts, level+1))\n",
    "    print(f\"Level {level} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raptor_client2 = chromadb.PersistentClient(path='database/raptor_backup.db')\n",
    "# raptor_db_backup = Chroma(client=raptor_client2, embedding_function=embd)\n",
    "\n",
    "for k,v in results.items():\n",
    "    texts = v[0]['text'].tolist()\n",
    "    batch_size = 100\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = leaf_texts[i:i+batch_size]\n",
    "\n",
    "        raptor_db.add_texts(batch_texts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
